{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d0fc9ca7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acc36449",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eff84ce1",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp=spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b99ee31",
   "metadata": {},
   "source": [
    "## Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "32a28ee0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corona PROPN nsubj\n",
      "will AUX aux\n",
      "go VERB ROOT\n",
      "very ADV advmod\n",
      "soon ADV advmod\n",
      ". PUNCT punct\n",
      "Do AUX aux\n",
      "not PART neg\n",
      "get VERB ROOT\n",
      "panic NOUN dobj\n",
      ", PUNCT punct\n",
      "maintain VERB conj\n",
      "social ADJ amod\n",
      "distancing NOUN dobj\n",
      "and CCONJ cc\n",
      "follow VERB conj\n",
      "the DET det\n",
      "instructions NOUN dobj\n",
      ". PUNCT punct\n",
      "Cases NOUN nsubj\n",
      "in ADP prep\n",
      "U.S. PROPN pobj\n",
      "have AUX aux\n",
      "reduced VERB ROOT\n",
      "in ADP prep\n",
      "last ADJ amod\n",
      "48 NUM nummod\n",
      "hours NOUN pobj\n"
     ]
    }
   ],
   "source": [
    "#Create a Doc object\n",
    "doc = nlp(u'Corona will go very soon. Do not get panic, maintain social distancing and follow the instructions. Cases in U.S. have reduced in last 48 hours')\n",
    "\n",
    "#Print each token separately\n",
    "for token in doc:\n",
    "    print(token.text, token.pos_, token.dep_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f72997cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('tok2vec', <spacy.pipeline.tok2vec.Tok2Vec at 0x17f12a260>),\n",
       " ('tagger', <spacy.pipeline.tagger.Tagger at 0x17f129120>),\n",
       " ('parser', <spacy.pipeline.dep_parser.DependencyParser at 0x17ed398c0>),\n",
       " ('attribute_ruler',\n",
       "  <spacy.pipeline.attributeruler.AttributeRuler at 0x17e2609c0>),\n",
       " ('lemmatizer', <spacy.lang.en.lemmatizer.EnglishLemmatizer at 0x17f2a7440>),\n",
       " ('ner', <spacy.pipeline.ner.EntityRecognizer at 0x17ed39d20>)]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp.pipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cd30215a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Apple PROPN nsubj\n",
      "is AUX aux\n",
      "looking VERB ROOT\n",
      "at ADP prep\n",
      "buying VERB pcomp\n",
      "U.K. PROPN dobj\n",
      "startup VERB dep\n",
      "for ADP prep\n",
      "$ SYM quantmod\n",
      "1 NUM compound\n",
      "billion NUM pobj\n"
     ]
    }
   ],
   "source": [
    "doc2 = nlp(\"Apple is looking at buying U.K. startup for $1 billion\")\n",
    "for token in doc2:\n",
    "    print(token.text, token.pos_, token.dep_)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33503bb0",
   "metadata": {},
   "source": [
    "## POS Tagging "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4f555ecb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Apple PROPN\n",
      "is AUX\n",
      "n't PART\n",
      "looking VERB\n",
      "at ADP\n",
      "buying VERB\n",
      "U.K. PROPN\n",
      "startup NOUN\n",
      ". PUNCT\n"
     ]
    }
   ],
   "source": [
    "doc4 = nlp(u\"Apple isn't looking at buying U.K. startup.\")\n",
    "for token in doc4:     \n",
    "    print(token.text, token.pos_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f625d7e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'proper noun'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spacy.explain('PROPN')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3d12a2c",
   "metadata": {},
   "source": [
    "## Span (slicing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "27d83821",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "at buying U.K."
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc5 = nlp(u\"Apple isn't looking at buying U.K. startup.\")\n",
    "sliced_text = doc5[4:7]\n",
    "sliced_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d377d064",
   "metadata": {},
   "source": [
    "## Sentence "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a034e59b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is the first sentence.\n",
      "This is another sentence.\n",
      "This is the last sentence.\n"
     ]
    }
   ],
   "source": [
    "doc6 = nlp(u'This is the first sentence. This is another sentence. This is the last sentence.')\n",
    "for sent in doc6.sents:\n",
    "    print(sent)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ab5363f",
   "metadata": {},
   "source": [
    "## Lemmatization\n",
    "Reduce the form of a word  by applying morphological analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "33b044f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I \t PRON \t 4690420944186131903 \t I\n",
      "am \t AUX \t 10382539506755952630 \t be\n",
      "a \t DET \t 11901859001352538922 \t a\n",
      "runner \t NOUN \t 12640964157389618806 \t runner\n",
      "running \t VERB \t 12767647472892411841 \t run\n",
      "in \t ADP \t 3002984154512732771 \t in\n",
      "a \t DET \t 11901859001352538922 \t a\n",
      "race \t NOUN \t 8048469955494714898 \t race\n",
      "because \t SCONJ \t 16950148841647037698 \t because\n",
      "I \t PRON \t 4690420944186131903 \t I\n",
      "love \t VERB \t 3702023516439754181 \t love\n",
      "to \t PART \t 3791531372978436496 \t to\n",
      "run \t VERB \t 12767647472892411841 \t run\n",
      "since \t SCONJ \t 10066841407251338481 \t since\n",
      "I \t PRON \t 4690420944186131903 \t I\n",
      "ran \t VERB \t 12767647472892411841 \t run\n",
      "today \t NOUN \t 11042482332948150395 \t today\n"
     ]
    }
   ],
   "source": [
    "doc1 = nlp(u\"I am a runner running in a race because I love to run since I ran today\")\n",
    "for token in doc1:\n",
    "    print(token.text, '\\t', token.pos_, '\\t', token.lemma, '\\t', token.lemma_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f222965",
   "metadata": {},
   "source": [
    "## Stop Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d992d973",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{\"'d\",\n",
       " \"'ll\",\n",
       " \"'m\",\n",
       " \"'re\",\n",
       " \"'s\",\n",
       " \"'ve\",\n",
       " 'a',\n",
       " 'about',\n",
       " 'above',\n",
       " 'across',\n",
       " 'after',\n",
       " 'afterwards',\n",
       " 'again',\n",
       " 'against',\n",
       " 'all',\n",
       " 'almost',\n",
       " 'alone',\n",
       " 'along',\n",
       " 'already',\n",
       " 'also',\n",
       " 'although',\n",
       " 'always',\n",
       " 'am',\n",
       " 'among',\n",
       " 'amongst',\n",
       " 'amount',\n",
       " 'an',\n",
       " 'and',\n",
       " 'another',\n",
       " 'any',\n",
       " 'anyhow',\n",
       " 'anyone',\n",
       " 'anything',\n",
       " 'anyway',\n",
       " 'anywhere',\n",
       " 'are',\n",
       " 'around',\n",
       " 'as',\n",
       " 'at',\n",
       " 'back',\n",
       " 'be',\n",
       " 'became',\n",
       " 'because',\n",
       " 'become',\n",
       " 'becomes',\n",
       " 'becoming',\n",
       " 'been',\n",
       " 'before',\n",
       " 'beforehand',\n",
       " 'behind',\n",
       " 'being',\n",
       " 'below',\n",
       " 'beside',\n",
       " 'besides',\n",
       " 'between',\n",
       " 'beyond',\n",
       " 'both',\n",
       " 'bottom',\n",
       " 'but',\n",
       " 'by',\n",
       " 'ca',\n",
       " 'call',\n",
       " 'can',\n",
       " 'cannot',\n",
       " 'could',\n",
       " 'did',\n",
       " 'do',\n",
       " 'does',\n",
       " 'doing',\n",
       " 'done',\n",
       " 'down',\n",
       " 'due',\n",
       " 'during',\n",
       " 'each',\n",
       " 'eight',\n",
       " 'either',\n",
       " 'eleven',\n",
       " 'else',\n",
       " 'elsewhere',\n",
       " 'empty',\n",
       " 'enough',\n",
       " 'even',\n",
       " 'ever',\n",
       " 'every',\n",
       " 'everyone',\n",
       " 'everything',\n",
       " 'everywhere',\n",
       " 'except',\n",
       " 'few',\n",
       " 'fifteen',\n",
       " 'fifty',\n",
       " 'first',\n",
       " 'five',\n",
       " 'for',\n",
       " 'former',\n",
       " 'formerly',\n",
       " 'forty',\n",
       " 'four',\n",
       " 'from',\n",
       " 'front',\n",
       " 'full',\n",
       " 'further',\n",
       " 'get',\n",
       " 'give',\n",
       " 'go',\n",
       " 'had',\n",
       " 'has',\n",
       " 'have',\n",
       " 'he',\n",
       " 'hence',\n",
       " 'her',\n",
       " 'here',\n",
       " 'hereafter',\n",
       " 'hereby',\n",
       " 'herein',\n",
       " 'hereupon',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'him',\n",
       " 'himself',\n",
       " 'his',\n",
       " 'how',\n",
       " 'however',\n",
       " 'hundred',\n",
       " 'i',\n",
       " 'if',\n",
       " 'in',\n",
       " 'indeed',\n",
       " 'into',\n",
       " 'is',\n",
       " 'it',\n",
       " 'its',\n",
       " 'itself',\n",
       " 'just',\n",
       " 'keep',\n",
       " 'last',\n",
       " 'latter',\n",
       " 'latterly',\n",
       " 'least',\n",
       " 'less',\n",
       " 'made',\n",
       " 'make',\n",
       " 'many',\n",
       " 'may',\n",
       " 'me',\n",
       " 'meanwhile',\n",
       " 'might',\n",
       " 'mine',\n",
       " 'more',\n",
       " 'moreover',\n",
       " 'most',\n",
       " 'mostly',\n",
       " 'move',\n",
       " 'much',\n",
       " 'must',\n",
       " 'my',\n",
       " 'myself',\n",
       " \"n't\",\n",
       " 'name',\n",
       " 'namely',\n",
       " 'neither',\n",
       " 'never',\n",
       " 'nevertheless',\n",
       " 'next',\n",
       " 'nine',\n",
       " 'no',\n",
       " 'nobody',\n",
       " 'none',\n",
       " 'noone',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'nothing',\n",
       " 'now',\n",
       " 'nowhere',\n",
       " 'n‘t',\n",
       " 'n’t',\n",
       " 'of',\n",
       " 'off',\n",
       " 'often',\n",
       " 'on',\n",
       " 'once',\n",
       " 'one',\n",
       " 'only',\n",
       " 'onto',\n",
       " 'or',\n",
       " 'other',\n",
       " 'others',\n",
       " 'otherwise',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'out',\n",
       " 'over',\n",
       " 'own',\n",
       " 'part',\n",
       " 'per',\n",
       " 'perhaps',\n",
       " 'please',\n",
       " 'put',\n",
       " 'quite',\n",
       " 'rather',\n",
       " 're',\n",
       " 'really',\n",
       " 'regarding',\n",
       " 'same',\n",
       " 'say',\n",
       " 'see',\n",
       " 'seem',\n",
       " 'seemed',\n",
       " 'seeming',\n",
       " 'seems',\n",
       " 'serious',\n",
       " 'several',\n",
       " 'she',\n",
       " 'should',\n",
       " 'show',\n",
       " 'side',\n",
       " 'since',\n",
       " 'six',\n",
       " 'sixty',\n",
       " 'so',\n",
       " 'some',\n",
       " 'somehow',\n",
       " 'someone',\n",
       " 'something',\n",
       " 'sometime',\n",
       " 'sometimes',\n",
       " 'somewhere',\n",
       " 'still',\n",
       " 'such',\n",
       " 'take',\n",
       " 'ten',\n",
       " 'than',\n",
       " 'that',\n",
       " 'the',\n",
       " 'their',\n",
       " 'them',\n",
       " 'themselves',\n",
       " 'then',\n",
       " 'thence',\n",
       " 'there',\n",
       " 'thereafter',\n",
       " 'thereby',\n",
       " 'therefore',\n",
       " 'therein',\n",
       " 'thereupon',\n",
       " 'these',\n",
       " 'they',\n",
       " 'third',\n",
       " 'this',\n",
       " 'those',\n",
       " 'though',\n",
       " 'three',\n",
       " 'through',\n",
       " 'throughout',\n",
       " 'thru',\n",
       " 'thus',\n",
       " 'to',\n",
       " 'together',\n",
       " 'too',\n",
       " 'top',\n",
       " 'toward',\n",
       " 'towards',\n",
       " 'twelve',\n",
       " 'twenty',\n",
       " 'two',\n",
       " 'under',\n",
       " 'unless',\n",
       " 'until',\n",
       " 'up',\n",
       " 'upon',\n",
       " 'us',\n",
       " 'used',\n",
       " 'using',\n",
       " 'various',\n",
       " 'very',\n",
       " 'via',\n",
       " 'was',\n",
       " 'we',\n",
       " 'well',\n",
       " 'were',\n",
       " 'what',\n",
       " 'whatever',\n",
       " 'when',\n",
       " 'whence',\n",
       " 'whenever',\n",
       " 'where',\n",
       " 'whereafter',\n",
       " 'whereas',\n",
       " 'whereby',\n",
       " 'wherein',\n",
       " 'whereupon',\n",
       " 'wherever',\n",
       " 'whether',\n",
       " 'which',\n",
       " 'while',\n",
       " 'whither',\n",
       " 'who',\n",
       " 'whoever',\n",
       " 'whole',\n",
       " 'whom',\n",
       " 'whose',\n",
       " 'why',\n",
       " 'will',\n",
       " 'with',\n",
       " 'within',\n",
       " 'without',\n",
       " 'would',\n",
       " 'yet',\n",
       " 'you',\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves',\n",
       " '‘d',\n",
       " '‘ll',\n",
       " '‘m',\n",
       " '‘re',\n",
       " '‘s',\n",
       " '‘ve',\n",
       " '’d',\n",
       " '’ll',\n",
       " '’m',\n",
       " '’re',\n",
       " '’s',\n",
       " '’ve'}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp.Defaults.stop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "96c3326a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp.vocab['fifteen'].is_stop"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "422a0e93",
   "metadata": {},
   "source": [
    "### Add a stop word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ef6f6284",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Add the word to the set of stop words. Use lowercase!\n",
    "nlp.Defaults.stop_words.add('btw') #alwasy use lowercase while adding the stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "55ee8dd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set the stop_word tag on the lexeme\n",
    "nlp.vocab['btw'].is_stop = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cee76e95",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp.vocab['btw'].is_stop"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19099e35",
   "metadata": {},
   "source": [
    "### Removing a stop word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a738ba26",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'without'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Input \u001b[0;32mIn [21]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m#Remove the word from the set of stop words\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mnlp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDefaults\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstop_words\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mremove\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mwithout\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'without'"
     ]
    }
   ],
   "source": [
    "#Remove the word from the set of stop words from the list\n",
    "nlp.Defaults.stop_words.remove('without')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "cc376215",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove the stop_word tag from the lexeme\n",
    "nlp.vocab['without'].is_stop = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "46ff6e45",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp.vocab['without'].is_stop"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbcf835a",
   "metadata": {},
   "source": [
    "## Vocabulary and matching"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5e33b45",
   "metadata": {},
   "source": [
    "### Rule-based matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4f66da32",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import the Matcher library\n",
    "from spacy.matcher import Matcher\n",
    "matcher = Matcher(nlp.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2d397992",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating patterns\n",
    "# We want to develop a matcher that find all forms of spelling of united states\n",
    "# pattern1 looks for a single token whose lowercase text reads ‘unitedstates’\n",
    "# pattern2 looks for two adjacent tokens that read ‘united’ and ‘states’ in that order\n",
    "# pattern3 looks for three adjacent tokens, with a middle token that can be any punctuation.*\n",
    "pattern1 = [{'LOWER': 'unitedstates'}]\n",
    "pattern2 = [{'LOWER': 'united'}, {'LOWER': 'states'}]\n",
    "pattern3 = [{'LOWER': 'united'}, {'IS_PUNCT': True}, {'LOWER': 'states'}]\n",
    "patterns=[pattern1,pattern2,pattern3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d3a9e606",
   "metadata": {},
   "outputs": [],
   "source": [
    "#add patterns to the matcher\n",
    "matcher.add('UnitedStates', patterns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "623db89f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applying the matcher\n",
    "doc = nlp(u'The United States of America is a country consisting of 50 independent states. The first constitution of the UnitedStates was adopted in 1788. The current United-States flag was designed by a high school student – Robert G. Heft.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "134fa45c",
   "metadata": {},
   "source": [
    "```matcher``` returns a list of tuples. Each tuple contains (ID for the matcher, start token, end token) that map to the span ```doc[start:end]```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "26ad2f2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(15845173719804281779, 1, 3), (15845173719804281779, 19, 20), (15845173719804281779, 27, 30)]\n"
     ]
    }
   ],
   "source": [
    "found_matches = matcher(doc)\n",
    "print(found_matches)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d8e4cdb",
   "metadata": {},
   "source": [
    "### Setting pattern options and quantifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "98976623",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Redefine the patterns:\n",
    "pattern1 = [{'LOWER': 'unitedstates'}]\n",
    "#Token rule is made optional by passing an 'OP:'*' argument. '\n",
    "pattern2 = [{'LOWER': 'united'}, {'IS_PUNCT': True, 'OP':'*'}, {'LOWER': 'states'}]\n",
    "\n",
    "# These quantifiers can be added to the OP key\n",
    "# !: Negate the pattern, by requiring it to match exactly 0 times\n",
    "# ?: Make the pattern optional, by allowing it to match 0 or 1 time\n",
    "# +: require the pattern to match 1 or more times\n",
    "# *: Allow the pattern to match zero or more times "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "a673cdc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove the old patterns to avoid duplication:\n",
    "matcher.remove('UnitedStates')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "35bb185a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Add the new set of patterns to the 'SolarPower' matcher:\n",
    "matcher.add('someNameToMatcher', [ pattern1, pattern2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "d651f383",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(14270899081666383025, 0, 3)]\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(u'United--States has the world’s largest coal reserves.')\n",
    "found_matches = matcher(doc)\n",
    "print(found_matches)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6baddbf",
   "metadata": {},
   "source": [
    "### Lemma as a key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "5580a334",
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern1 = [{'LOWER': 'solarpower'}]\n",
    "pattern2 = [{'LOWER': 'solar'}, {'IS_PUNCT': True, 'OP':'*'}, {'LEMMA': 'power'}] # CHANGE THIS PATTERN\n",
    "#Remove the old patterns to avoid duplication:\n",
    "matcher.remove('someNameToMatcher') #remove the previously added matcher name\n",
    "#Add the new set of patterns to the 'SolarPower' matcher:\n",
    "matcher.add('SolarPower',[pattern1, pattern2])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "2412d5c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(8656102463236116519, 0, 3), (8656102463236116519, 5, 8)]\n"
     ]
    }
   ],
   "source": [
    "doc2 = nlp(u'Solar-powered energy runs solar-powered cars.')\n",
    "found_matches = matcher(doc2)\n",
    "print(found_matches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "912c19a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Other tokens that can be put in\n",
    "# ORTH - The exact verbatim text of a token\n",
    "# LOWER - The lowercase form of a token text\n",
    "# LENGTH - the length of the token text\n",
    "# IS_ALPHA, IS ASCII, IS_DIGIT - Token text consists of alphanumeric, characters, ASCII characters, digits\n",
    "# IS_LOWER, IS_UPPER, IS_TITLE - Token text in lowercase, uppercase,titlecase\n",
    "# IS_PUNCT, IS_SPACE, IS_STOP - Token is punctuation, whitespace, stop word\n",
    "# LIKE_NUM, LIKE_URL, LIKE_EMAIL - Token text resembles a number, URL, email \n",
    "# POS, TAG, DEP, LEMMA, SHAPE - The token's simple and extended POS tag, dependency label, lemma, shape\n",
    "# ENT_TYPE - The token's entity label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9052b2f",
   "metadata": {},
   "source": [
    "### Token wildcard\n",
    "`[{'ORTH':'#'],{}]` matches whatever token follow the hashtag"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e89a9b28",
   "metadata": {},
   "source": [
    "## Phrase Matcher\n",
    "The PhraseMatcher lets you efficiently match large terminology lists. While the Matcher lets you match sequences based on lists of token descriptions, the PhraseMatcher accepts match patterns in the form of Doc objects. \n",
    "\n",
    "--> Instead of matching a token, match a phrase that's turned into an NLP doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "10747af6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Perform standard imports, reset nlp\n",
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "572bc68f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "orange\n",
      "apple\n"
     ]
    }
   ],
   "source": [
    "# Import the PhraseMatcher library\n",
    "from spacy.matcher import PhraseMatcher\n",
    "matcher = PhraseMatcher(nlp.vocab)\n",
    "#create the list of words to match\n",
    "fruit_list = ['apple','orange','banana',]\n",
    "#obtain doc object for each word in the list and store it in a list\n",
    "patterns = [nlp(fruit) for fruit in fruit_list]\n",
    "#add the pattern to the matcher\n",
    "matcher.add(\"FRUIT_PATTERN\", patterns)\n",
    "#process some text\n",
    "doc = nlp(\"An orange contains citric acid and an apple contains oxalic acid\")\n",
    "matches = matcher(doc)\n",
    "for match_id, start, end in matches:\n",
    "    span = doc[start:end]\n",
    "    print(span.text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "1b1fc691",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matched! [(7732777389095836264, 0, 2)]\n"
     ]
    }
   ],
   "source": [
    "def on_match(matcher, doc, id, matches):\n",
    "    print('Matched!', matches)\n",
    "\n",
    "matcher = PhraseMatcher(nlp.vocab)\n",
    "matcher.add(\"OBAMA\", [nlp(\"Barack Obama\")], on_match=on_match)\n",
    "matcher.add(\"HEALTH\", [nlp(\"health care reform\"), nlp(\"healthcare reform\")], on_match=on_match)\n",
    "doc = nlp(\"Barack Obama urges Congress to find courage to defend his healthcare reforms\")\n",
    "matches = matcher(doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed8eedf4",
   "metadata": {},
   "source": [
    "## Viewing Token \n",
    "spaCy encodes all strings to hash values to reduce memory usage and improve efficiency. So to get the readable string representation of an attribute, we need to add an underscore _ to its name: Note that token.pos and token.tag return integer hash values; by adding the underscores we get the text equivalent that lives in doc.vocab.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "115c4833",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp=spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "f8c6e733",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc=nlp(u\"The quick brown fox jumped over the lazy dog's back.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "7adb075c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The        DET        DT         determiner\n",
      "quick      ADJ        JJ         adjective (English), other noun-modifier (Chinese)\n",
      "brown      ADJ        JJ         adjective (English), other noun-modifier (Chinese)\n",
      "fox        NOUN       NN         noun, singular or mass\n",
      "jumped     VERB       VBD        verb, past tense\n",
      "over       ADP        IN         conjunction, subordinating or preposition\n",
      "the        DET        DT         determiner\n",
      "lazy       ADJ        JJ         adjective (English), other noun-modifier (Chinese)\n",
      "dog        NOUN       NN         noun, singular or mass\n",
      "'s         PART       POS        possessive ending\n",
      "back       NOUN       NN         noun, singular or mass\n",
      ".          PUNCT      .          punctuation mark, sentence closer\n"
     ]
    }
   ],
   "source": [
    "#{10} is the space between\n",
    "for token in doc:\n",
    "    print(f'{token.text:{10}} {token.pos_:{10}} {token.tag_:{10}} {spacy.explain(token.tag_)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84c98e1c",
   "metadata": {},
   "source": [
    "### Counting POS Tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "01abf91d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{90: 2, 84: 3, 92: 3, 100: 1, 85: 1, 94: 1, 97: 1}"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "POS_counts=doc.count_by(spacy.attrs.POS)\n",
    "POS_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "3814b24f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'DET'"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#DET apperas twice in the doc\n",
    "doc.vocab[90].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "ffe8980c",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Create a frequency list of POS tags from the entire document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "0be8fe57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "84. ADJ  : 3\n",
      "85. ADP  : 1\n",
      "90. DET  : 2\n",
      "92. NOUN : 3\n",
      "94. PART : 1\n",
      "97. PUNCT: 1\n",
      "100. VERB : 1\n"
     ]
    }
   ],
   "source": [
    "for k,v in sorted (POS_counts.items()):\n",
    "    print(f'{k}. {doc.vocab[k].text:{5}}: {v}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "8aee7518",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "402. amod: 3\n",
      "415. det : 2\n",
      "429. nsubj: 1\n",
      "439. pobj: 1\n",
      "440. poss: 1\n",
      "443. prep: 1\n",
      "445. punct: 1\n",
      "8110129090154140942. case: 1\n",
      "8206900633647566924. ROOT: 1\n"
     ]
    }
   ],
   "source": [
    "DEP_counts = doc.count_by(spacy.attrs.DEP)\n",
    "for k,v in sorted (DEP_counts.items()):\n",
    "    print(f'{k}. {doc.vocab[k].text:{4}}: {v}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caf0559c",
   "metadata": {},
   "source": [
    "## Visualising POS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "090b6a1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy import displacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "dfa4a75d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" xml:lang=\"en\" id=\"a1d7ffce62cf40f8b8edb6bb6f97c473-0\" class=\"displacy\" width=\"1095\" height=\"327.0\" direction=\"ltr\" style=\"max-width: none; height: 327.0px; color: #000000; background: #ffffff; font-family: Arial; direction: ltr\">\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"237.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"50\">The</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"50\">DET</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"237.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"145\">quick</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"145\">ADJ</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"237.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"240\">brown</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"240\">ADJ</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"237.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"335\">fox</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"335\">NOUN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"237.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"430\">jumped</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"430\">VERB</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"237.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"525\">over</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"525\">ADP</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"237.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"620\">the</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"620\">DET</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"237.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"715\">lazy</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"715\">ADJ</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"237.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"810\">dog</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"810\">NOUN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"237.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"905\">'s</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"905\">PART</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"237.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1000\">back.</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1000\">NOUN</tspan>\n",
       "</text>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-a1d7ffce62cf40f8b8edb6bb6f97c473-0-0\" stroke-width=\"2px\" d=\"M70,192.0 C70,49.5 330.0,49.5 330.0,192.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-a1d7ffce62cf40f8b8edb6bb6f97c473-0-0\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">det</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M70,194.0 L62,182.0 78,182.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-a1d7ffce62cf40f8b8edb6bb6f97c473-0-1\" stroke-width=\"2px\" d=\"M165,192.0 C165,97.0 325.0,97.0 325.0,192.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-a1d7ffce62cf40f8b8edb6bb6f97c473-0-1\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">amod</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M165,194.0 L157,182.0 173,182.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-a1d7ffce62cf40f8b8edb6bb6f97c473-0-2\" stroke-width=\"2px\" d=\"M260,192.0 C260,144.5 320.0,144.5 320.0,192.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-a1d7ffce62cf40f8b8edb6bb6f97c473-0-2\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">amod</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M260,194.0 L252,182.0 268,182.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-a1d7ffce62cf40f8b8edb6bb6f97c473-0-3\" stroke-width=\"2px\" d=\"M355,192.0 C355,144.5 415.0,144.5 415.0,192.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-a1d7ffce62cf40f8b8edb6bb6f97c473-0-3\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">nsubj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M355,194.0 L347,182.0 363,182.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-a1d7ffce62cf40f8b8edb6bb6f97c473-0-4\" stroke-width=\"2px\" d=\"M450,192.0 C450,144.5 510.0,144.5 510.0,192.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-a1d7ffce62cf40f8b8edb6bb6f97c473-0-4\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">prep</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M510.0,194.0 L518.0,182.0 502.0,182.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-a1d7ffce62cf40f8b8edb6bb6f97c473-0-5\" stroke-width=\"2px\" d=\"M640,192.0 C640,97.0 800.0,97.0 800.0,192.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-a1d7ffce62cf40f8b8edb6bb6f97c473-0-5\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">det</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M640,194.0 L632,182.0 648,182.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-a1d7ffce62cf40f8b8edb6bb6f97c473-0-6\" stroke-width=\"2px\" d=\"M735,192.0 C735,144.5 795.0,144.5 795.0,192.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-a1d7ffce62cf40f8b8edb6bb6f97c473-0-6\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">amod</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M735,194.0 L727,182.0 743,182.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-a1d7ffce62cf40f8b8edb6bb6f97c473-0-7\" stroke-width=\"2px\" d=\"M830,192.0 C830,97.0 990.0,97.0 990.0,192.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-a1d7ffce62cf40f8b8edb6bb6f97c473-0-7\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">poss</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M830,194.0 L822,182.0 838,182.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-a1d7ffce62cf40f8b8edb6bb6f97c473-0-8\" stroke-width=\"2px\" d=\"M830,192.0 C830,144.5 890.0,144.5 890.0,192.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-a1d7ffce62cf40f8b8edb6bb6f97c473-0-8\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">case</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M890.0,194.0 L898.0,182.0 882.0,182.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-a1d7ffce62cf40f8b8edb6bb6f97c473-0-9\" stroke-width=\"2px\" d=\"M545,192.0 C545,2.0 1000.0,2.0 1000.0,192.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-a1d7ffce62cf40f8b8edb6bb6f97c473-0-9\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">pobj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1000.0,194.0 L1008.0,182.0 992.0,182.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "</svg></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "displacy.render(doc,style='dep',jupyter=True,options={'distance':95})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "9b7b4235",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The        DET     det     determiner\n",
      "quick      ADJ     amod    adjectival modifier\n",
      "brown      ADJ     amod    adjectival modifier\n",
      "fox        NOUN    nsubj   nominal subject\n",
      "jumped     VERB    ROOT    None\n",
      "over       ADP     prep    prepositional modifier\n",
      "the        DET     det     determiner\n",
      "lazy       ADJ     amod    adjectival modifier\n",
      "dog        NOUN    poss    possession modifier\n",
      "'s         PART    case    case marking\n",
      "back       NOUN    pobj    object of preposition\n",
      ".          PUNCT   punct   punctuation\n"
     ]
    }
   ],
   "source": [
    "for token in doc: \n",
    "    print(f'{token.text:{10}} {token.pos_:{7}} {token.dep_:{7}} {spacy.explain(token.dep_)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73afcac3",
   "metadata": {},
   "outputs": [],
   "source": [
    "https://ashutoshtripathi.com/2020/04/27/named-entity-recognition-ner-using-spacy-nlp-part-4/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
